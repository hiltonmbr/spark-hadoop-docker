# Spark Cluster on top of Hadoop Cluster - Docker Environment

## Overview

Apache Hadoop is used to process and analyze large datasets. Apache Spark is a powerful open-source processing engine built around speed, ease of use, and sophisticated analytics.

This project provides an easy way to deploy a Spark Cluster on top of Hadoop cluster using Docker containers and python language for study and experimentation.
